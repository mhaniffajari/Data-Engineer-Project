{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "import os\r\n",
        "import pyspark\r\n",
        "import datetime\r\n",
        "\r\n",
        "from pyspark.sql.functions import *\r\n",
        "from datetime import *\r\n",
        "from delta.tables import *\r\n",
        "from pyspark.sql.window import Window\r\n",
        "from notebookutils import mssparkutils\r\n",
        "from pyspark.sql import functions as F, SparkSession"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Main Class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Class Construct Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "class PathConstructor:\r\n",
        "    def __init__(self,*args):\r\n",
        "        self.filepath = args\r\n",
        "    def pathconstructor(self):\r\n",
        "        if isinstance(self.filepath,(tuple)):\r\n",
        "            command = \"\"\r\n",
        "            for i in range(len(self.filepath)-2):\r\n",
        "                command += \"/%s\"\r\n",
        "            path = \"abfss://%s@%s.dfs.core.windows.net\"\r\n",
        "            pathcombine = path+command\r\n",
        "\r\n",
        "            defaultPath = pathcombine %(self.filepath)\r\n",
        "        return defaultPath "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Class Reading File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "class ReadFile:\r\n",
        "\r\n",
        "    def __init__(self, path, file_format, delimiter, with_header):\r\n",
        "        self.path = path\r\n",
        "        self.file_format = file_format\r\n",
        "        self.delimiter = delimiter\r\n",
        "        self.with_header = with_header\r\n",
        "\r\n",
        "    def readfrompath(self):\r\n",
        "        df = spark.read.format(self.file_format) \\\r\n",
        "                .option(\"header\", self.with_header) \\\r\n",
        "                .option('delimiter', self.delimiter) \\\r\n",
        "                .option(\"inferSchema\", \"true\") \\\r\n",
        "                .load(self.path)\r\n",
        "        return (df)\r\n",
        "        \r\n",
        "    def readfrompath_withschema(self, schema):\r\n",
        "        df = spark.read.format(self.file_format) \\\r\n",
        "                .option(\"header\", self.with_header) \\\r\n",
        "                .option('delimiter', self.delimiter) \\\r\n",
        "                .schema(schema) \\\r\n",
        "                .load(self.path)\r\n",
        "        return (df)\r\n",
        "    \r\n",
        "    def readfrompath_excel(self, sheetInformation):\r\n",
        "        df = spark.read.format(\"com.crealytics.spark.excel\") \\\r\n",
        "                .option(\"header\", self.with_header) \\\r\n",
        "                .option(\"inferSchema\", \"true\") \\\r\n",
        "                .option(\"dataAddress\", sheetInformation) \\\r\n",
        "                .load(self.path)\r\n",
        "        return (df)\r\n",
        "    \r\n",
        "    def readfrompath_excel_withschema(self, sheetInformation, schema):\r\n",
        "        df = spark.read.format(\"com.crealytics.spark.excel\") \\\r\n",
        "                .option(\"header\", self.with_header) \\\r\n",
        "                .option(\"inferSchema\", \"true\") \\\r\n",
        "                .option(\"dataAddress\", sheetInformation) \\\r\n",
        "                .schema(schema) \\\r\n",
        "                .load(self.path)\r\n",
        "        return (df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Class Filtering Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "class GetDelta:\r\n",
        "    def __init__(self,data,partiotionCol,dropCol,flagCol,orderByCol,changeFlag):\r\n",
        "        self.data = data\r\n",
        "        self.partiotionCol = partiotionCol\r\n",
        "        self.dropCol = dropCol\r\n",
        "        self.flagCol = flagCol\r\n",
        "        self.orderByCol = orderByCol\r\n",
        "        self.changeFlag = changeFlag\r\n",
        "    def GetDelta(self):\r\n",
        "        df_filter = self.data.where(F.col(self.flagCol).isin(self.changeFlag))\\\r\n",
        "        .drop(*self.dropCol).withColumnRenamed(self.flagCol,'change_flag')\r\n",
        "        df = df_filter.withColumn('rank',F.dense_rank() \\\r\n",
        "        .over(Window.partitionBy(*self.partiotionCol))\\\r\n",
        "        .orderBy(F.desc(self.orderByCol)))\\\r\n",
        "        .where('rank == 1').drop(self.orderByCol,'rank')\r\n",
        "        return(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Class Merging Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "class MergingConstructor:\r\n",
        "    def __init__(self,keyList,updateList):\r\n",
        "        self.keyList = keyList\r\n",
        "        self.updateList = updateList\r\n",
        "    def Construct_Upsert(self):\r\n",
        "        mainString = 'MERGE INTO old_data USING new_data ON'\r\n",
        "        i = 0\r\n",
        "        k = 0\r\n",
        "\r\n",
        "        for key in self.keyList:\r\n",
        "            if i ==0:\r\n",
        "                keyString = 'old_data.' + key + ' = ' + 'new_data.' + key\r\n",
        "            else :\r\n",
        "                keyString = 'old_data.'+ key + ' = ' + 'new_data.' + key\r\n",
        "            mainString = mainString + keyString\r\n",
        "            i = i+1\r\n",
        "        mainString = mainString + 'WHEN NOT MATCHED THEN INSERT *'\r\n",
        "        return(mainString)\r\n",
        "\r\n",
        "    def Construct_Delete(self):\r\n",
        "        mainString = 'MERGE INTO old_data USING new_data ON'\r\n",
        "        i = 0\r\n",
        "\r\n",
        "        for key in keyList:\r\n",
        "            if i == 0:\r\n",
        "                keyString = 'old_data.'+ key + ' = ' +'new_data.'+key\r\n",
        "            else:\r\n",
        "                keyString = 'AND old_data.'+ key + '=' + 'new_data.'+key\r\n",
        "            mainString = mainString + keyString\r\n",
        "            i = i+1\r\n",
        "        mainString = mainString + 'WHEN MATCHED THEN DELETE'\r\n",
        "        return(mainString)\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Merge Process Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "class MergeClass:\r\n",
        "    def __init__(self,df_old,df_new):\r\n",
        "        self.df_old = df_old\r\n",
        "        self.df_new = df_new\r\n",
        "    def merge(self,mergeString):\r\n",
        "        if self.df_new.count() > 0:\r\n",
        "            self.df_old.registerTempTable('old_data')\r\n",
        "            self.df_new.registerTempTable('new_data')\r\n",
        "            try:\r\n",
        "                print(mergeString)\r\n",
        "                spark.sql(mergeString)\r\n",
        "                print('Merge Success')\r\n",
        "            except Exception as e:\r\n",
        "                print('Error')\r\n",
        "                print('Error Happened: ',e)\r\n",
        "        else:\r\n",
        "            print('No new data')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Class Directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "class DirectoryClass:\r\n",
        "    def __init__(self,path):\r\n",
        "        self.path = path\r\n",
        "    def isDirExist(self):\r\n",
        "        try:\r\n",
        "            mssparkutils.fs.ls(self.path)\r\n",
        "            return True\r\n",
        "        except Exception as e:\r\n",
        "            return False\r\n",
        "    def CreateDir(self):\r\n",
        "        mssparkutils.fs.mkdirs(self.path)\r\n",
        "    def RemoveDir(self,additional_path):\r\n",
        "        if additional_path != '':\r\n",
        "            removePath = self.path + '/' + additional_path\r\n",
        "        else:\r\n",
        "            removePath = self.path\r\n",
        "        mssparkutils.fs.rm(removePath)\r\n",
        "    def CopyDirectory(self,sourcePath,isRecurse:bool):\r\n",
        "        mssparkutils.fs.cp(sourcePath,self.path,isRecurse)\r\n",
        "    def MoveDirectory(self,sourcePath,isRecurse:bool):\r\n",
        "        mssparkutils.fs.mv(sourcePath,self.path,isRecurse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Config Date Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "class Config_Date:\r\n",
        "    def date_YYYYMM(self):\r\n",
        "        now = datetime.now()\r\n",
        "        return now.strftime('%Y%m')\r\n",
        "    def date_YYYMMDD(self):\r\n",
        "        now = datetime.now()\r\n",
        "        return now.strftime('%Y%m%d')\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Class Partition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "class Partition:\r\n",
        "    def partition_YYYYMM(self):\r\n",
        "        range_list = []\r\n",
        "        for i in range(30):\r\n",
        "            yymm = (datetime.now()-timedelta(days=i))\r\n",
        "            range_list.append([yymm.year,yymm.month])\r\n",
        "        schema_lst = ['year','month']\r\n",
        "        return spark.createDataFrame(range_list,schema_lst).dropDuplicates().collect()\r\n",
        "    def partition_YYYY(self):\r\n",
        "        range_list = []\r\n",
        "        yymm_0 = datetime.now()\r\n",
        "        yymm_1 = datetime.now() - timedelta(days=365)\r\n",
        "        range_list.append([yymm_0])\r\n",
        "        range_list.append([yymm_1])\r\n",
        "        schema_lst = ['year']\r\n",
        "        return spark.createDataFrame(range_list,schema_lst).dropDuplicates().collect()"
      ]
    }
  ],
  "metadata": {
    "description": "global_class",
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "python"
    },
    "language_info": {
      "name": "python"
    }
  }
}